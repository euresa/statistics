{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05a6be89",
   "metadata": {},
   "source": [
    "# Scraping Wikipedia Pages\n",
    "### Author: Sam Eure\n",
    "### Last updated: May 14, 2021\n",
    "#### [Code in github](https://github.com/euresa/statistics/blob/master/Python%20Projects/Wikipedia_Scraping/wiki_scraper.ipynb)\n",
    "\n",
    "In this notebook, I created a short Python script for scraping Wikipedia pages. My WikiScraper class focuses on associating natural text paragraphs and hyperlinks to their associated sections/subsections on any given Wikipedia page. In the first part of this script I develop the scraping class for gathering and processing the data. Later on in this notebook I perform some basic NLP on the text that I got online."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c72781",
   "metadata": {},
   "source": [
    "## Scraping and Organizing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cacb150",
   "metadata": {},
   "source": [
    "### Classes, Functions, and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f777d8f6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class WikiScraper:\n",
    "    \"\"\"A class used for scraping Wikipedia page data. Organizes natural language on\n",
    "    a Wikipedia page and creates a list of the hyperlinks present on the page.\"\"\"\n",
    "\n",
    "    def __init__(self, subject: str=\"\", link: str=\"\", printing: bool=False) -> None:\n",
    "        self.hyperlinks: list = []\n",
    "        self.para_list: list = []\n",
    "        self.section_headers: list = []\n",
    "        if subject != \"\":\n",
    "            self.HTML_LINK: str = _make_wiki_link(subject)\n",
    "            self._get_wiki_data(self.HTML_LINK, printing=printing)\n",
    "            self._get_wiki_df()\n",
    "        elif link != \"\":\n",
    "            self.HTML_LINK = link\n",
    "            self._get_wiki_data(self.HTML_LINK, printing=printing)\n",
    "            self._get_wiki_df()\n",
    "        else:\n",
    "            print(\"Please provide a 'subject' or 'link' argument.\")\n",
    "\n",
    "    def _get_wiki_data(self, HTML_LINK: str, printing: bool=False) -> None:\n",
    "        \"\"\"Takes in a Wikipedia link and returns the natural language paragraphs,\n",
    "        section titles, and hyperlinks found in the document.\n",
    "        \"\"\"\n",
    "        if self.section_headers == []:\n",
    "            soup: BeautifulSoup = _get_soup_doc(HTML_LINK)\n",
    "            headers: list = _get_headers(soup)\n",
    "            header_strings = [str(h) for h in headers]\n",
    "            h_indices: list = _get_indices(soup, header_strings)\n",
    "\n",
    "            self.subject: str = _get_title(soup)\n",
    "            printer_count = 5  # counter in the for loop below\n",
    "            if printing:\n",
    "                print(\"PAGE TITLE:\", self.subject)\n",
    "                printer_count = 0\n",
    "\n",
    "            for i, head in enumerate(headers):\n",
    "                self.section_headers.append(head.text)\n",
    "                paragraphs: list = _get_raw_paragraphs(soup, h_indices, i)\n",
    "                hrefs = [_get_hyperlink(p) for p in paragraphs]\n",
    "                paras = [_remove_HTML(p) for p in paragraphs]\n",
    "                self.hyperlinks.append(combine(hrefs))\n",
    "                self.para_list.append(paras)\n",
    "                if printer_count < 3:  # To limit printing\n",
    "                    printer_count = printer_count + 1\n",
    "                    print(\"\\n\", \"#\" * 100, \"\\n\\tSECTION:\", head.text)\n",
    "                    for p in paras:\n",
    "                        print(\"\\n\", _show_some_text(p))\n",
    "                    print(\"\\nLINKS:\", _show_some_text(str(combine(hrefs))))\n",
    "        else:\n",
    "            print(\"Wiki data already scraped.\")\n",
    "\n",
    "    def _get_wiki_df(self):\n",
    "        \"\"\"Organizes Wikipedia page data into a Pandas dataframe.\"\"\"\n",
    "        wiki_df = pd.DataFrame(\n",
    "            {\n",
    "                \"section\": self.section_headers,\n",
    "                \"hyperlinks\": self.hyperlinks,\n",
    "                \"paragraphs\": self.para_list,\n",
    "            }\n",
    "        )\n",
    "        wiki_df[\"words\"] = wiki_df.apply(\n",
    "            lambda row: _get_words_from_paragraphs(row.paragraphs), axis=1\n",
    "        )\n",
    "        wiki_df = _add_count_feature(wiki_df, \"hyperlinks\")\n",
    "        wiki_df = _add_count_feature(wiki_df, \"paragraphs\")\n",
    "        wiki_df = _add_count_feature(wiki_df, \"words\")\n",
    "        wiki_df = wiki_df.set_index(\"section\")\n",
    "        self.df = wiki_df[~(wiki_df.paragraphs_count == 0)]\n",
    "\n",
    "\n",
    "def _make_wiki_link(subject: str) -> str:\n",
    "    \"\"\"Takes in a string containing a subject (e.g. Python, Cats, Russia, etc.) and\n",
    "    returns the related Wikipedia page link to the subject.\n",
    "    \n",
    "        example:\n",
    "        >>> _make_wiki_link('Statistics')\n",
    "        https://en.wikipedia.org/wiki/Statistics\n",
    "    \"\"\"\n",
    "    link = \"https://en.wikipedia.org/wiki/\" + subject\n",
    "    return link\n",
    "\n",
    "\n",
    "def _get_soup_doc(HTML_LINK: str, parser: str=\"html.parser\") -> BeautifulSoup:\n",
    "    \"\"\"Takes in an html link and returns a BeautifulSoup document.\"\"\"\n",
    "    response = requests.get(HTML_LINK)\n",
    "    soup_doc = BeautifulSoup(response.content, \"html.parser\")\n",
    "    return soup_doc\n",
    "\n",
    "\n",
    "def _get_title(soup_doc: BeautifulSoup) -> str:\n",
    "    \"\"\"Returns the title of the web page.\"\"\"\n",
    "    title = soup_doc.find(id=\"firstHeading\").text\n",
    "    return title\n",
    "\n",
    "\n",
    "def _get_headers(soup: BeautifulSoup) -> list:\n",
    "    \"\"\"Returns the header of each section in a Wikipedia page.\"\"\"\n",
    "    headers = soup.find_all(\"span\", attrs=\"mw-headline\")\n",
    "    return headers\n",
    "\n",
    "\n",
    "def _remove_footnotes(text: str) -> str:\n",
    "    \"\"\"Drop footnote superscripts in brackets\"\"\"\n",
    "    text = re.sub(r\"\\[.*?\\]+\", \"\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def _get_indices(soup: BeautifulSoup, string_elements: list) -> list:\n",
    "    \"\"\"Returns the a list of the starting index for each element in a list of strings.\"\"\"\n",
    "    soup_string = str(soup)\n",
    "    indices = [soup_string.index(string) for string in string_elements]\n",
    "    return indices\n",
    "\n",
    "\n",
    "def _get_index(text: str, element: str) -> int:\n",
    "    \"\"\"Attempts to get the index of an element. Returns -1 if not present\n",
    "\n",
    "        example:\n",
    "            >>> _get_index(\"hello world\", \"w\")\n",
    "            6\n",
    "            >>> _get_index(\"hello world\", \"z\")\n",
    "            -1\n",
    "    \"\"\"\n",
    "    try:\n",
    "        idx = text.index(element)\n",
    "        return idx\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "\n",
    "def combine(lists):\n",
    "    \"\"\"Combines a list of lists into one list to return.\n",
    "\n",
    "        example:\n",
    "        >>>combine([[1, 2, 3], [4, 5, 6], [7], [8, 9]])\n",
    "        [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "    \"\"\"\n",
    "    combo = []\n",
    "    for list_ in lists:\n",
    "        combo.extend(list_)\n",
    "    return combo\n",
    "\n",
    "\n",
    "def _collect_pattern_pairs(text: str, start_char: str, end_char: str) -> list:\n",
    "    \"\"\"Returns a list of all text between sets of start_char and end_char characters.\"\"\"\n",
    "    collection = []\n",
    "    while _get_index(text, start_char) != -1:\n",
    "        start: int = _get_index(text, start_char)\n",
    "        end: int = _get_index(text[start:], end_char) + start  # make sure end is after start\n",
    "        collection.append(text[start : end + len(end_char)])\n",
    "        text = text[end + len(end_char) :]\n",
    "    return collection\n",
    "\n",
    "\n",
    "def _get_raw_paragraphs(soup: BeautifulSoup, h_indices: list, i: int) -> list:\n",
    "    \"\"\"Returns unprocessed HTML paragraphs.\"\"\"\n",
    "    soup_string = str(soup)\n",
    "    try:\n",
    "        raw_text_i = soup_string[h_indices[i] : h_indices[i + 1]]\n",
    "    except:\n",
    "        raw_text_i = soup_string[h_indices[i] :]\n",
    "    paragraphs: list = _collect_pattern_pairs(raw_text_i, \"<p>\", \"</p>\")\n",
    "    return paragraphs\n",
    "\n",
    "\n",
    "def _get_paragraphs(soup: BeautifulSoup, h_indices: list, i: int) -> list:\n",
    "    \"\"\"Returns the text paragraphs associated with the section header specified by an index.\"\"\"\n",
    "    paragraphs: list = _get_raw_paragraphs(soup, h_indices, i)\n",
    "    clean_paragraphs = [_remove_HTML(p) for p in paragraphs]\n",
    "    return clean_paragraphs\n",
    "\n",
    "\n",
    "def _remove_pattern_pair(text: str, start_char: str, end_char: str) -> str:\n",
    "    \"\"\"Removes all text between the start_char and end_char and returns remaining text.\"\"\"\n",
    "    while _get_index(text, start_char) != -1:\n",
    "        start: int = _get_index(text, start_char)\n",
    "        end: int = _get_index(text[start:], end_char) + start  # make sure end is after start\n",
    "        text = text[:start] + text[end + len(end_char) :]\n",
    "    return text\n",
    "\n",
    "\n",
    "def _remove_substrings(text: str, substring_list: list) -> str:\n",
    "    \"\"\"Removes all occurences of all substrings in a list from a text string. Returns new string.\"\"\"\n",
    "    for pattern in substring_list:\n",
    "        text = re.sub(pattern, \"\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def _remove_HTML(text: str) -> str:\n",
    "    \"\"\"Removes the HTML elements from a substring of an HTML document and returns resulting string.\"\"\"\n",
    "    to_remove = [\"</a>\", \"</sup>\", \"<p>\", \"</p>\"]\n",
    "    text = _remove_substrings(text, to_remove)\n",
    "    plain_text: str = _remove_pattern_pair(text, \"<\", \">\")\n",
    "    plain_text = _remove_footnotes(plain_text)\n",
    "    return plain_text\n",
    "\n",
    "\n",
    "def _clean_wiki_links(hlinks: list) -> list:\n",
    "    \"\"\"Completes hyperlinks to other Wikipedia pages.\"\"\"\n",
    "    for i, link in enumerate(hlinks):\n",
    "        if \"/wiki/\" in link:\n",
    "            end = link.index('\"')\n",
    "            hlinks[i] = re.sub(\"/wiki/\", \"https://en.wikipedia.org/wiki/\", link[:end])\n",
    "    return hlinks\n",
    "\n",
    "\n",
    "def _remove_internal_links(hlinks: list) -> list:\n",
    "    \"\"\"Removes links that reference different parts of the Wikipedia page.\"\"\"\n",
    "    for link in hlinks:\n",
    "        if \"#\" == link[0]:\n",
    "            hlinks.remove(link)\n",
    "    return hlinks\n",
    "\n",
    "\n",
    "def _clean_cite_notes(hlinks: list) -> list:\n",
    "    \"\"\"Removes references to links cited at the bottom of the Wikipedia page.\"\"\"\n",
    "    cite_notes = []\n",
    "    for i, link in enumerate(hlinks):\n",
    "        if \"#cite_note-\" in link:\n",
    "            cite_notes.append(hlinks[i])\n",
    "    for c in cite_notes:\n",
    "        hlinks.remove(c)\n",
    "    return hlinks\n",
    "\n",
    "\n",
    "def _clean_hyperlinks(html_hyperlinks: list) -> list:\n",
    "    \"\"\"Removes the HTML markup around hyperlinks and returns a list of hyperlinks\"\"\"\n",
    "    to_remove = ['<a href=\"', '\">']\n",
    "    hlinks = [_remove_substrings(link, to_remove) for link in html_hyperlinks]\n",
    "    hlinks = _clean_wiki_links(hlinks)\n",
    "    hlinks = _clean_cite_notes(hlinks)\n",
    "    hlinks = _remove_internal_links(hlinks)\n",
    "    return hlinks\n",
    "\n",
    "\n",
    "def _get_hyperlink(text: str) -> list:\n",
    "    \"\"\"Returns a list of all hyperlinks included in a subsection of an HTML document.\"\"\"\n",
    "    html_hyperlinks: list = _collect_pattern_pairs(text, \"<a href=\", \">\")\n",
    "    hyperlinks: list = _clean_hyperlinks(html_hyperlinks)\n",
    "    return hyperlinks\n",
    "\n",
    "\n",
    "def _show_some_text(text: str) -> str:\n",
    "    \"\"\"Returns first 100 characters in string.\"\"\"\n",
    "    return text[:80] + \"...\"\n",
    "\n",
    "\n",
    "def _get_words_from_paragraphs(p_list: list) -> list:\n",
    "    \"\"\"Returns a list of words from a list of paragraphs.\"\"\"\n",
    "    paragraph: str = \" \\n \".join(p_list) #paragraphs are separated by ' \\n '\n",
    "    return paragraph.split(\" \")\n",
    "\n",
    "\n",
    "def _add_count_feature(df: pd.DataFrame, feature: str) -> pd.DataFrame:\n",
    "    \"\"\"Returns df with new feature that is the length of the list of a different feature.\"\"\"\n",
    "    df[feature + \"_count\"] = df.apply(lambda row: len(row[feature]), axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9615f168",
   "metadata": {},
   "source": [
    "### Scraping and processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28208f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAGE TITLE: Statistics\n",
      "\n",
      " #################################################################################################### \n",
      "\tSECTION: Introduction\n",
      "\n",
      " Statistics is a mathematical body of science that pertains to the collection, an...\n",
      "\n",
      " In applying statistics to a problem, it is common practice to start with a popul...\n",
      "\n",
      " When a census is not feasible, a chosen subset of the population called a sample...\n",
      "\n",
      "LINKS: ['https://en.wikipedia.org/wiki/Data', 'https://en.wikipedia.org/wiki/Mathematic...\n",
      "\n",
      " #################################################################################################### \n",
      "\tSECTION: Mathematical statistics\n",
      "\n",
      " Mathematical statistics is the application of mathematics to statistics. Mathema...\n",
      "\n",
      "LINKS: ['https://en.wikipedia.org/wiki/Mathematics', 'https://en.wikipedia.org/wiki/Mat...\n",
      "\n",
      " #################################################################################################### \n",
      "\tSECTION: History\n",
      "\n",
      " The early writings on statistical inference date back to Arab mathematicians and...\n",
      "\n",
      " The earliest European writing on statistics dates back to 1663, with the publica...\n",
      "\n",
      " The mathematical foundations of modern statistics were laid in the 17th century ...\n",
      "\n",
      " The modern field of statistics emerged in the late 19th and early 20th century i...\n",
      "\n",
      " Ronald Fisher coined the term null hypothesis during the Lady tasting tea experi...\n",
      "\n",
      " The second wave of the 1910s and 20s was initiated by William Sealy Gosset, and ...\n",
      "\n",
      " The final wave, which mainly saw the refinement and expansion of earlier develop...\n",
      "\n",
      " Today, statistical methods are applied in all fields that involve decision makin...\n",
      "\n",
      "LINKS: ['https://en.wikipedia.org/wiki/Mathematics_in_medieval_Islam', 'https://en.wiki...\n"
     ]
    }
   ],
   "source": [
    "#Choose your subject of interest!\n",
    "SUBJECT = \"Statistics\"\n",
    "wiki_obj = WikiScraper(subject=SUBJECT, printing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a818d81",
   "metadata": {},
   "source": [
    "Some of the sections don't have any paragraphs associated with them. This is because I assigned paragraphs to more specific subsections as opposed to sections as a whole."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5eab0d",
   "metadata": {},
   "source": [
    "## Data is organized as a Pandas dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d95486a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hyperlinks</th>\n",
       "      <th>paragraphs</th>\n",
       "      <th>words</th>\n",
       "      <th>hyperlinks_count</th>\n",
       "      <th>paragraphs_count</th>\n",
       "      <th>words_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>section</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Introduction</th>\n",
       "      <td>[https://en.wikipedia.org/wiki/Data, https://e...</td>\n",
       "      <td>[Statistics is a mathematical body of science ...</td>\n",
       "      <td>[Statistics, is, a, mathematical, body, of, sc...</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mathematical statistics</th>\n",
       "      <td>[https://en.wikipedia.org/wiki/Mathematics, ht...</td>\n",
       "      <td>[Mathematical statistics is the application of...</td>\n",
       "      <td>[Mathematical, statistics, is, the, applicatio...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>History</th>\n",
       "      <td>[https://en.wikipedia.org/wiki/Mathematics_in_...</td>\n",
       "      <td>[The early writings on statistical inference d...</td>\n",
       "      <td>[The, early, writings, on, statistical, infere...</td>\n",
       "      <td>51</td>\n",
       "      <td>8</td>\n",
       "      <td>758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sampling</th>\n",
       "      <td>[https://en.wikipedia.org/wiki/Design_of_exper...</td>\n",
       "      <td>[When full census data cannot be collected, st...</td>\n",
       "      <td>[When, full, census, data, cannot, be, collect...</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Experimental and observational studies</th>\n",
       "      <td>[https://en.wikipedia.org/wiki/Causality, http...</td>\n",
       "      <td>[A common goal for a statistical research proj...</td>\n",
       "      <td>[A, common, goal, for, a, statistical, researc...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                               hyperlinks  \\\n",
       "section                                                                                     \n",
       "Introduction                            [https://en.wikipedia.org/wiki/Data, https://e...   \n",
       "Mathematical statistics                 [https://en.wikipedia.org/wiki/Mathematics, ht...   \n",
       "History                                 [https://en.wikipedia.org/wiki/Mathematics_in_...   \n",
       "Sampling                                [https://en.wikipedia.org/wiki/Design_of_exper...   \n",
       "Experimental and observational studies  [https://en.wikipedia.org/wiki/Causality, http...   \n",
       "\n",
       "                                                                               paragraphs  \\\n",
       "section                                                                                     \n",
       "Introduction                            [Statistics is a mathematical body of science ...   \n",
       "Mathematical statistics                 [Mathematical statistics is the application of...   \n",
       "History                                 [The early writings on statistical inference d...   \n",
       "Sampling                                [When full census data cannot be collected, st...   \n",
       "Experimental and observational studies  [A common goal for a statistical research proj...   \n",
       "\n",
       "                                                                                    words  \\\n",
       "section                                                                                     \n",
       "Introduction                            [Statistics, is, a, mathematical, body, of, sc...   \n",
       "Mathematical statistics                 [Mathematical, statistics, is, the, applicatio...   \n",
       "History                                 [The, early, writings, on, statistical, infere...   \n",
       "Sampling                                [When, full, census, data, cannot, be, collect...   \n",
       "Experimental and observational studies  [A, common, goal, for, a, statistical, researc...   \n",
       "\n",
       "                                        hyperlinks_count  paragraphs_count  \\\n",
       "section                                                                      \n",
       "Introduction                                          18                 3   \n",
       "Mathematical statistics                                3                 1   \n",
       "History                                               51                 8   \n",
       "Sampling                                              10                 3   \n",
       "Experimental and observational studies                10                 1   \n",
       "\n",
       "                                        words_count  \n",
       "section                                              \n",
       "Introduction                                    346  \n",
       "Mathematical statistics                          27  \n",
       "History                                         758  \n",
       "Sampling                                        242  \n",
       "Experimental and observational studies          201  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_obj.df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4049c8",
   "metadata": {},
   "source": [
    "Now that I have the plain text from each of the sections, I can do some natural language processing to find the most popular words from each section. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15f486f",
   "metadata": {},
   "source": [
    "## NLP\n",
    "\n",
    "I'll do some basic NLP to find popular words within documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3566cdd6",
   "metadata": {},
   "source": [
    "### Functions and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bb48d8a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "I'll use spaCy to help with some of the natural language processing,\n",
    "such as identifying 'stop words'.\n",
    "\"\"\"\n",
    "from spacy import load\n",
    "\n",
    "\n",
    "def _remove_punctuation(words: list) -> list:\n",
    "    \"\"\"Removes punctuation and special characters.\"\"\"\n",
    "    punc_list = [\n",
    "        \".\",\n",
    "        \",\",\n",
    "        \")\",\n",
    "        \"(\",\n",
    "        \"/\",\n",
    "        \"]\",\n",
    "        \"[\",\n",
    "        \"\\n\",\n",
    "        \" \",\n",
    "        \";\",\n",
    "        \":\",\n",
    "        '\"',\n",
    "        \"'\",\n",
    "        \"\\n \\n \",\n",
    "        \"-\",\n",
    "    ]\n",
    "    no_punc = [w for w in words if w not in punc_list]\n",
    "    return no_punc\n",
    "\n",
    "\n",
    "def _get_top_n_strings(str_list: list, n: int=3) -> list:\n",
    "    \"\"\"Finds most popular n strings in a string list. Returns a list of tuples (word, count).\"\"\"\n",
    "    word_df: pd.DataFrame = pd.DataFrame({\"words\": str_list})\n",
    "    word_vc = word_df.value_counts()\n",
    "    top_strings = [(word_vc.index[i][0], word_vc[i]) for i in range(n)]\n",
    "    return top_strings\n",
    "\n",
    "\n",
    "def _remove_stop_words(doc, lemmas: bool=False) -> list:\n",
    "    \"\"\"Removes \"stop words\" (common words like \"is\", \"but\", \"and\") from list of words.\"\"\"\n",
    "    if lemmas:\n",
    "        # Lemmas are the base form of a word. Ex: the lemma of swimming is swim.\n",
    "        interesting_words = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    else:\n",
    "        interesting_words = [token.text for token in doc if not token.is_stop]\n",
    "    return interesting_words\n",
    "\n",
    "\n",
    "def find_popular_words(nlp, words_list: list, n: int=3, lemmas: bool=False) -> list:\n",
    "    \"\"\"Finds the most popular words that aren't stop words in a list of words.\"\"\"\n",
    "    text = \" \".join(words_list)\n",
    "    doc = nlp(text.lower())\n",
    "    nice_words: list = _remove_stop_words(doc, lemmas=lemmas)\n",
    "    actual_nice_words = _remove_punctuation(nice_words)\n",
    "    popular_words: list = _get_top_n_strings(actual_nice_words, n=n)\n",
    "    return popular_words\n",
    "\n",
    "\n",
    "def find_top_words(nlp, df, n: int=1, lemmas: bool=False) -> pd.Series:\n",
    "    \"\"\"Returns a pandas.Series object of tuples comprised of the the top 'n' words \n",
    "    from each row of a pandas.DataFrame already containing a 'words' feature.\n",
    "    \"\"\"\n",
    "    top_words: pd.Series = df.apply(\n",
    "        lambda row: find_popular_words(nlp, row.words, n=n, lemmas=lemmas), axis=1\n",
    "    )\n",
    "    return top_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4a87a4",
   "metadata": {},
   "source": [
    "### Finding most common words in each section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "284ad56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words_count</th>\n",
       "      <th>top words</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>section</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Introduction</th>\n",
       "      <td>346</td>\n",
       "      <td>[(data, 16), (population, 8), (statistics, 7)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mathematical statistics</th>\n",
       "      <td>27</td>\n",
       "      <td>[(mathematical, 3), (analysis, 2), (statistics...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>History</th>\n",
       "      <td>758</td>\n",
       "      <td>[(statistics, 11), (statistical, 9), (fisher, 6)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sampling</th>\n",
       "      <td>242</td>\n",
       "      <td>[(population, 7), (sample, 6), (theory, 5)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Experimental and observational studies</th>\n",
       "      <td>201</td>\n",
       "      <td>[(studies, 6), (variables, 4), (study, 4)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        words_count  \\\n",
       "section                                               \n",
       "Introduction                                    346   \n",
       "Mathematical statistics                          27   \n",
       "History                                         758   \n",
       "Sampling                                        242   \n",
       "Experimental and observational studies          201   \n",
       "\n",
       "                                                                                top words  \n",
       "section                                                                                    \n",
       "Introduction                               [(data, 16), (population, 8), (statistics, 7)]  \n",
       "Mathematical statistics                 [(mathematical, 3), (analysis, 2), (statistics...  \n",
       "History                                 [(statistics, 11), (statistical, 9), (fisher, 6)]  \n",
       "Sampling                                      [(population, 7), (sample, 6), (theory, 5)]  \n",
       "Experimental and observational studies         [(studies, 6), (variables, 4), (study, 4)]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load language model\n",
    "nlp = load('en_core_web_sm')\n",
    "\n",
    "wiki_obj.df['top words'] = find_top_words(nlp, wiki_obj.df, n=3)\n",
    "print(wiki_obj.subject)\n",
    "wiki_obj.df[['words_count','top words']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335e7c26",
   "metadata": {},
   "source": [
    "We can also determine the top (most frequent) words for the entire topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dc46263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most popular words for the 'statistics' Wikipedia page are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('statistics', 69),\n",
       " ('data', 57),\n",
       " ('statistical', 56),\n",
       " ('sample', 32),\n",
       " ('probability', 30)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The most popular words for the 'statistics' Wikipedia page are:\")\n",
    "find_popular_words(nlp, combine(list(wiki_obj.df.words.values)), n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34151326",
   "metadata": {},
   "source": [
    "Unsurprisingly, the most popular words are'statistics' and 'data'!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d8c0ca",
   "metadata": {},
   "source": [
    "The dataframe above is from the 'Statistics' page on Wikipedia. Let's check out some of the pages that are referenced on this page using the hyperlinks we found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c1d34f7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://en.wikipedia.org/wiki/Standard_deviation',\n",
       " 'https://en.wikipedia.org/wiki/Longitude',\n",
       " 'https://en.wikipedia.org/wiki/Fahrenheit',\n",
       " 'https://en.wikipedia.org/wiki/Boolean_data_type',\n",
       " 'https://en.wikipedia.org/wiki/Karl_Pearson']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "allLinks = combine(wiki_obj.hyperlinks)\n",
    "random.shuffle(allLinks)\n",
    "allLinks[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94074d7b",
   "metadata": {},
   "source": [
    "### Random walk on graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f808054e",
   "metadata": {},
   "source": [
    "We can easily get a new page of Wikipedia data using these links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0474c4bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard deviation\n",
      "Variance\n",
      "Classical mechanics\n",
      "Angular momentum\n",
      "Closed system\n",
      "Thermodynamic system\n",
      "Quantum thermodynamics\n",
      "Third law of thermodynamics\n",
      "Spin glass\n",
      "Phase transition\n"
     ]
    }
   ],
   "source": [
    "new_page = allLinks[0]\n",
    "for _ in range(10):\n",
    "    new_page = WikiScraper(link=allLinks[0])\n",
    "    edges = combine(new_page.hyperlinks)\n",
    "    print(new_page.subject)\n",
    "    random.shuffle(edges)\n",
    "    allLinks = edges"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
