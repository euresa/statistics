{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05a6be89",
   "metadata": {},
   "source": [
    "# Scraping Wikipedia Pages\n",
    "### Author: Sam Eure\n",
    "### Last updated: May 14, 2021\n",
    "#### [Code in github](https://github.com/euresa/statistics/blob/master/Python%20Projects/Wikipedia_Scraping/wiki_scraper.ipynb)\n",
    "\n",
    "In this notebook, I created a short Python script for scraping Wikipedia pages in order to obtain the hyperlinks listed in each section of the Wikipedia pages. Given a link to a Wikipedia page, this script should do the following:\n",
    "\n",
    "1) Find the title of the page \n",
    "\n",
    "2) Find the header of each section\n",
    "\n",
    "3) Gather the plain text found in each section\n",
    "\n",
    "4) Grab all the hyperlinks referenced in the paragraphs of each section (if present)\n",
    "\n",
    "In the second part of this notebook, I organize this data into a Pandas dataframe and then do some basic NLP on the documents I found."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c72781",
   "metadata": {},
   "source": [
    "## Scraping and Organizing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cacb150",
   "metadata": {},
   "source": [
    "### Classes, Functions, and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f777d8f6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import requests  \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "class WikiScraper:\n",
    "    '''A class used for scraping Wikipedia page data. Organizes natural language on page and hyperlinks.'''\n",
    "    def __init__(self, subject=None, link=None, printing=False):\n",
    "        self.hyperlinks = []\n",
    "        self.para_list = []\n",
    "        self.section_headers = []\n",
    "        if subject is not None:\n",
    "            self.HTML_LINK = make_wiki_link(subject)\n",
    "            self.get_wiki_data(self.HTML_LINK, printing=printing)\n",
    "            self.get_wiki_df()\n",
    "        elif link is not None:\n",
    "            self.HTML_LINK = link\n",
    "            self.get_wiki_data(self.HTML_LINK, printing=printing)\n",
    "            self.get_wiki_df()\n",
    "        else:\n",
    "            print(\"Please provide a 'subject' or 'link' argument.\")\n",
    "        \n",
    "        \n",
    "    def get_wiki_data(self, HTML_LINK, printing = False):\n",
    "        '''Takes in a Wikipedia link and returns the natural language paragraphs, \n",
    "            section titles, and hyperlinks found in the document.\n",
    "        '''\n",
    "        if self.section_headers == []:\n",
    "            soup = get_soup_doc(HTML_LINK)\n",
    "            headers = get_headers(soup)\n",
    "            header_strings = [str(h) for h in headers]\n",
    "            h_indices = get_indices(soup, header_strings)\n",
    "\n",
    "            self.subject = get_title(soup)\n",
    "            printer_count = 5 #counter in the for loop below\n",
    "            if printing:\n",
    "                print('PAGE TITLE:', self.subject)\n",
    "                printer_count = 0\n",
    "\n",
    "            for i, head in enumerate(headers):\n",
    "                self.section_headers.append(head.text)\n",
    "                paragraphs = get_raw_paragraphs(soup, h_indices, i)\n",
    "                hrefs = [get_hyperlink(p) for p in paragraphs]\n",
    "                paras = [remove_HTML(p) for p in paragraphs]\n",
    "                self.hyperlinks.append(combine(hrefs))\n",
    "                self.para_list.append(paras)\n",
    "                if printer_count < 3: #To limit printing\n",
    "                    printer_count = printer_count + 1\n",
    "                    print(\"\\n\",\"#\"*100, '\\n\\tSECTION:', head.text)\n",
    "                    for p in paras:\n",
    "                        print('\\n', show_some_text(p))\n",
    "                    print('\\nLINKS:', show_some_text(str(combine(hrefs))))\n",
    "        else:\n",
    "            print(\"Wiki data already scraped.\")\n",
    "            \n",
    "    def get_wiki_df(self):\n",
    "        '''Organizes Wikipedia page data into a Pandas dataframe.'''\n",
    "        wiki_df = pd.DataFrame({'section': self.section_headers, \"hyperlinks\": self.hyperlinks, 'paragraphs': self.para_list})\n",
    "        wiki_df['words'] = wiki_df.apply(lambda row : get_words_from_paragraphs(row.paragraphs), axis=1)\n",
    "        wiki_df = add_count_feature(wiki_df, 'hyperlinks')\n",
    "        wiki_df = add_count_feature(wiki_df, 'paragraphs')\n",
    "        wiki_df = add_count_feature(wiki_df, 'words')\n",
    "        wiki_df = wiki_df.set_index('section')\n",
    "        self.df = wiki_df[~(wiki_df.paragraphs_count==0)]\n",
    "\n",
    "\n",
    "    \n",
    "def make_wiki_link(subject):\n",
    "    link = \"https://en.wikipedia.org/wiki/\"+ subject\n",
    "    return(link)\n",
    "\n",
    "def get_soup_doc(HTML_LINK, parser = 'html.parser'):\n",
    "    '''Takes in an html link and returns a BeautifulSoup document.'''\n",
    "    response = requests.get(HTML_LINK)\n",
    "    soup_doc = BeautifulSoup(response.content, 'html.parser')\n",
    "    return(soup_doc)\n",
    "\n",
    "def get_title(soup_doc):\n",
    "    '''Returns the title of the web page.'''\n",
    "    title = soup_doc.find(id='firstHeading').text\n",
    "    return(title)\n",
    "\n",
    "def get_headers(soup):\n",
    "    '''Returns the header of each section in a Wikipedia page.'''\n",
    "    headers = soup.find_all('span', attrs='mw-headline')\n",
    "    return(headers)\n",
    "\n",
    "def remove_footnotes(text):\n",
    "    '''Drop footnote superscripts in brackets'''\n",
    "    text = re.sub(r\"\\[.*?\\]+\", '', text)\n",
    "    return(text)\n",
    "\n",
    "def get_indices(soup, string_elements):\n",
    "    '''Returns the a list of the starting index for each element in a list of strings.'''\n",
    "    soup_string = str(soup)\n",
    "    indices = [soup_string.index(string) for string in string_elements]\n",
    "    return(indices)\n",
    "\n",
    "def get_index(text, element):\n",
    "    '''Attempts to get the index of an element. Returns None if not present'''\n",
    "    try:\n",
    "        idx = text.index(element)\n",
    "        return(idx)\n",
    "    except:\n",
    "        return(None)\n",
    "    \n",
    "def combine(lists):\n",
    "    '''Combines a list of lists into one list to return.'''\n",
    "    combo = []\n",
    "    for list_ in lists:\n",
    "        combo.extend(list_)\n",
    "    return(combo)\n",
    "\n",
    "def collect_pattern_pairs(text, start_char, end_char):\n",
    "    '''Returns a list of all text between sets of start_char and end_char characters.'''\n",
    "    collection = []\n",
    "    while get_index(text, start_char) is not None:\n",
    "        start = get_index(text, start_char)\n",
    "        end = get_index(text[start:], end_char) + start #make sure end is after start\n",
    "        collection.append(text[start:end+len(end_char)])\n",
    "        text = text[end+len(end_char):]\n",
    "    return(collection)\n",
    "\n",
    "def get_raw_paragraphs(soup, h_indices, i):\n",
    "    '''Returns unprocessed HTML paragraphs.'''\n",
    "    soup_string = str(soup)\n",
    "    try:\n",
    "        raw_text_i = soup_string[h_indices[i]:h_indices[i+1]]\n",
    "    except:\n",
    "        raw_text_i = soup_string[h_indices[i]:]\n",
    "    paragraphs = collect_pattern_pairs(raw_text_i, \"<p>\", \"</p>\")\n",
    "    return(paragraphs)\n",
    "\n",
    "def get_paragraphs(soup, h_indices, i):\n",
    "    '''Returns the text paragraphs associated with the section header specified by an index.'''\n",
    "    paragraphs = get_raw_paragraphs(soup, h_indices, i)\n",
    "    clean_paragraphs = [remove_HTML(p) for p in paragraphs]\n",
    "    return(clean_paragraphs)\n",
    "\n",
    "def remove_pattern_pair(text, start_char, end_char):\n",
    "    '''Removes all text between the start_char and end_char and returns remaining text.'''\n",
    "    while get_index(text, start_char) is not None:\n",
    "        start = get_index(text, start_char)\n",
    "        end = get_index(text[start:], end_char) + start #make sure end is after start\n",
    "        text = text[:start] + text[end+len(end_char):]\n",
    "    return(text)\n",
    "\n",
    "def remove_substrings(text, substring_list):\n",
    "    '''Removes all occurences of all substrings in a list from a text string. Returns new string.'''\n",
    "    for pattern in substring_list:\n",
    "        text = re.sub(pattern, \"\", text)\n",
    "    return(text)\n",
    "\n",
    "def remove_HTML(text):\n",
    "    '''Removes the HTML elements from a substring of an HTML document and returns resulting string.'''\n",
    "    to_remove = ['</a>', '</sup>', '<p>', '</p>']\n",
    "    text = remove_substrings(text, to_remove)\n",
    "    plain_text = remove_pattern_pair(text, '<', \">\")\n",
    "    plain_text = remove_footnotes(plain_text)\n",
    "    return(plain_text)\n",
    "\n",
    "def clean_wiki_links(hlinks):\n",
    "    '''Completes hyperlinks to other Wikipedia pages.'''\n",
    "    for i, link in enumerate(hlinks):\n",
    "        if '/wiki/' in link:\n",
    "            end = link.index('\"')\n",
    "            hlinks[i] = re.sub('/wiki/', 'https://en.wikipedia.org/wiki/', link[:end])\n",
    "    return(hlinks)\n",
    "\n",
    "def remove_internal_links(hlinks):\n",
    "    '''Removes links that reference different parts of the Wikipedia page.'''\n",
    "    internal_links = []\n",
    "    for link in hlinks:\n",
    "        if \"#\" == link[0]:\n",
    "            hlinks.remove(link)\n",
    "    return(hlinks)\n",
    "    \n",
    "def clean_cite_notes(hlinks):\n",
    "    '''Removes references to links cited at the bottom of the Wikipedia page.'''\n",
    "    cite_notes = []\n",
    "    for i, link in enumerate(hlinks):\n",
    "        if '#cite_note-' in link:\n",
    "            cite_notes.append(hlinks[i])\n",
    "    for c in cite_notes:\n",
    "        hlinks.remove(c)\n",
    "    return(hlinks)\n",
    "\n",
    "def clean_hyperlinks(html_hyperlinks):\n",
    "    '''Removes the HTML markup around hyperlinks and returns a list of hyperlinks'''\n",
    "    to_remove =['<a href=\"', '\">']\n",
    "    hlinks = [remove_substrings(link, to_remove) for link in html_hyperlinks]\n",
    "    hlinks = clean_wiki_links(hlinks)\n",
    "    hlinks = clean_cite_notes(hlinks)\n",
    "    hlinks = remove_internal_links(hlinks)\n",
    "    return(hlinks)\n",
    "\n",
    "def get_hyperlink(text):\n",
    "    '''Returns a list of all hyperlinks included in a subsection of an HTML document.'''\n",
    "    html_hyperlinks = collect_pattern_pairs(text, '<a href=', '>')\n",
    "    hyperlinks = clean_hyperlinks(html_hyperlinks)\n",
    "    return(hyperlinks)\n",
    "\n",
    "def show_some_text(text):\n",
    "    '''Returns first 100 characters in string.'''\n",
    "    return(text[:80]+\"...\")\n",
    "\n",
    "def join_strings(string_list, join_char = ' \\n '):\n",
    "    \"\"\"Joins a list of strings into one string.\"\"\"\n",
    "    return(join_char.join(string_list))\n",
    "\n",
    "def get_words_from_paragraphs(p_list):\n",
    "    '''Returns a list of words from a list of paragraphs.'''\n",
    "    paragraph = join_strings(p_list)\n",
    "    return(paragraph.split(\" \"))\n",
    "\n",
    "def add_count_feature(df, feature):\n",
    "    '''Returns df with new feature that is the length of the list of a different feature.'''\n",
    "    df[feature+\"_count\"] = df.apply(lambda row : len(row[feature]), axis=1)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9615f168",
   "metadata": {},
   "source": [
    "### Scraping and processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28208f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAGE TITLE: Statistics\n",
      "\n",
      " #################################################################################################### \n",
      "\tSECTION: Introduction\n",
      "\n",
      " Statistics is a mathematical body of science that pertains to the collection, an...\n",
      "\n",
      " In applying statistics to a problem, it is common practice to start with a popul...\n",
      "\n",
      " When a census is not feasible, a chosen subset of the population called a sample...\n",
      "\n",
      "LINKS: ['https://en.wikipedia.org/wiki/Data', 'https://en.wikipedia.org/wiki/Mathematic...\n",
      "\n",
      " #################################################################################################### \n",
      "\tSECTION: Mathematical statistics\n",
      "\n",
      " Mathematical statistics is the application of mathematics to statistics. Mathema...\n",
      "\n",
      "LINKS: ['https://en.wikipedia.org/wiki/Mathematics', 'https://en.wikipedia.org/wiki/Mat...\n",
      "\n",
      " #################################################################################################### \n",
      "\tSECTION: History\n",
      "\n",
      " The early writings on statistical interference date back to Arab mathematicians ...\n",
      "\n",
      " The earliest European writing on statistics dates back to 1663, with the publica...\n",
      "\n",
      " The mathematical foundations of modern statistics were laid in the 17th century ...\n",
      "\n",
      " The modern field of statistics emerged in the late 19th and early 20th century i...\n",
      "\n",
      " Ronald Fisher coined the term null hypothesis during the Lady tasting tea experi...\n",
      "\n",
      " The second wave of the 1910s and 20s was initiated by William Sealy Gosset, and ...\n",
      "\n",
      " The final wave, which mainly saw the refinement and expansion of earlier develop...\n",
      "\n",
      " Today, statistical methods are applied in all fields that involve decision makin...\n",
      "\n",
      "LINKS: ['https://en.wikipedia.org/wiki/Mathematics_in_medieval_Islam', 'https://en.wiki...\n"
     ]
    }
   ],
   "source": [
    "#Choose your subject of interest!\n",
    "SUBJECT = \"Statistics\"\n",
    "wiki_obj = WikiScraper(subject=SUBJECT, printing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a818d81",
   "metadata": {},
   "source": [
    "Some of the sections don't have any paragraphs associated with them. This is because I assigned paragraphs to more specific subsections as opposed to sections as a whole."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5eab0d",
   "metadata": {},
   "source": [
    "## Data is organized into a Pandas dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d95486a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hyperlinks</th>\n",
       "      <th>paragraphs</th>\n",
       "      <th>words</th>\n",
       "      <th>hyperlinks_count</th>\n",
       "      <th>paragraphs_count</th>\n",
       "      <th>words_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>section</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Introduction</th>\n",
       "      <td>[https://en.wikipedia.org/wiki/Data, https://e...</td>\n",
       "      <td>[Statistics is a mathematical body of science ...</td>\n",
       "      <td>[Statistics, is, a, mathematical, body, of, sc...</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mathematical statistics</th>\n",
       "      <td>[https://en.wikipedia.org/wiki/Mathematics, ht...</td>\n",
       "      <td>[Mathematical statistics is the application of...</td>\n",
       "      <td>[Mathematical, statistics, is, the, applicatio...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>History</th>\n",
       "      <td>[https://en.wikipedia.org/wiki/Mathematics_in_...</td>\n",
       "      <td>[The early writings on statistical interferenc...</td>\n",
       "      <td>[The, early, writings, on, statistical, interf...</td>\n",
       "      <td>51</td>\n",
       "      <td>8</td>\n",
       "      <td>758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sampling</th>\n",
       "      <td>[https://en.wikipedia.org/wiki/Design_of_exper...</td>\n",
       "      <td>[When full census data cannot be collected, st...</td>\n",
       "      <td>[When, full, census, data, cannot, be, collect...</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Experimental and observational studies</th>\n",
       "      <td>[https://en.wikipedia.org/wiki/Causality, http...</td>\n",
       "      <td>[A common goal for a statistical research proj...</td>\n",
       "      <td>[A, common, goal, for, a, statistical, researc...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                               hyperlinks  \\\n",
       "section                                                                                     \n",
       "Introduction                            [https://en.wikipedia.org/wiki/Data, https://e...   \n",
       "Mathematical statistics                 [https://en.wikipedia.org/wiki/Mathematics, ht...   \n",
       "History                                 [https://en.wikipedia.org/wiki/Mathematics_in_...   \n",
       "Sampling                                [https://en.wikipedia.org/wiki/Design_of_exper...   \n",
       "Experimental and observational studies  [https://en.wikipedia.org/wiki/Causality, http...   \n",
       "\n",
       "                                                                               paragraphs  \\\n",
       "section                                                                                     \n",
       "Introduction                            [Statistics is a mathematical body of science ...   \n",
       "Mathematical statistics                 [Mathematical statistics is the application of...   \n",
       "History                                 [The early writings on statistical interferenc...   \n",
       "Sampling                                [When full census data cannot be collected, st...   \n",
       "Experimental and observational studies  [A common goal for a statistical research proj...   \n",
       "\n",
       "                                                                                    words  \\\n",
       "section                                                                                     \n",
       "Introduction                            [Statistics, is, a, mathematical, body, of, sc...   \n",
       "Mathematical statistics                 [Mathematical, statistics, is, the, applicatio...   \n",
       "History                                 [The, early, writings, on, statistical, interf...   \n",
       "Sampling                                [When, full, census, data, cannot, be, collect...   \n",
       "Experimental and observational studies  [A, common, goal, for, a, statistical, researc...   \n",
       "\n",
       "                                        hyperlinks_count  paragraphs_count  \\\n",
       "section                                                                      \n",
       "Introduction                                          18                 3   \n",
       "Mathematical statistics                                3                 1   \n",
       "History                                               51                 8   \n",
       "Sampling                                              10                 3   \n",
       "Experimental and observational studies                10                 1   \n",
       "\n",
       "                                        words_count  \n",
       "section                                              \n",
       "Introduction                                    346  \n",
       "Mathematical statistics                          27  \n",
       "History                                         758  \n",
       "Sampling                                        242  \n",
       "Experimental and observational studies          201  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_obj.df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4049c8",
   "metadata": {},
   "source": [
    "Now that I have the plain text from each of the sections, I can do some natural language processing to find the most popular words from each section. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15f486f",
   "metadata": {},
   "source": [
    "## NLP\n",
    "\n",
    "I'll do some basic NLP to find popular words within documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3566cdd6",
   "metadata": {},
   "source": [
    "### Functions and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bb48d8a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#I'll use spaCy to help with some of the natural language processing, such as identifying 'stop words'.\n",
    "import spacy\n",
    "\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    '''Removes punctuation and special characters.'''\n",
    "    punc_list = ['.', ',', ')', '(', '/', ']', '[', '\\n', ' ', ';', ':', '\"',\"'\", '\\n \\n ', '-']\n",
    "    no_punc = [w for w in words if w not in punc_list]\n",
    "    return(no_punc)\n",
    "\n",
    "def get_top_n_strings(str_list, n=3):\n",
    "    '''Finds most popular n strings in a string list. Returns a list of tuples (word, count).'''\n",
    "    word_df = pd.DataFrame({'words': str_list})\n",
    "    word_vc = word_df.value_counts()\n",
    "    top_strings = [(word_vc.index[i][0], word_vc[i]) for i in range(n)]\n",
    "    return(top_strings)\n",
    "\n",
    "def remove_stop_words(doc, lemmas=False):\n",
    "    '''Removes \"stop words\" (common words like \"is\", \"but\", \"and\") from list of words.'''\n",
    "    if lemmas:\n",
    "        #Lemmas are the base form of a word. Ex: the lemma of swimming is swim.\n",
    "        interesting_words = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    else:\n",
    "        interesting_words = [token.text for token in doc if not token.is_stop]\n",
    "    return(interesting_words)\n",
    "\n",
    "def find_popular_words(nlp, words_list, n=3, lemmas=False):\n",
    "    \"\"\"Finds the most popular words that aren't stop words in a list of words.\"\"\"\n",
    "    text = join_strings(words_list, join_char=\" \")\n",
    "    doc = nlp(text.lower()) #make lowercase\n",
    "    nice_words = remove_stop_words(doc, lemmas=lemmas)\n",
    "    actual_nice_words = remove_punctuation(nice_words)\n",
    "    popular_words = get_top_n_strings(actual_nice_words, n=n)\n",
    "    return(popular_words)\n",
    "\n",
    "def find_top_words(nlp, df, n=1, lemmas=False):\n",
    "    '''Adds new feature listing tuples of \"n\" top words.'''\n",
    "    top_words= df.apply(lambda row : find_popular_words(nlp, row.words, n=n, lemmas=lemmas), axis=1)\n",
    "    return(top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4a87a4",
   "metadata": {},
   "source": [
    "### Finding most common words in each section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "284ad56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words_count</th>\n",
       "      <th>top words</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>section</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Introduction</th>\n",
       "      <td>346</td>\n",
       "      <td>[(data, 16), (population, 8), (statistics, 7)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mathematical statistics</th>\n",
       "      <td>27</td>\n",
       "      <td>[(mathematical, 3), (analysis, 2), (statistics...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>History</th>\n",
       "      <td>758</td>\n",
       "      <td>[(statistics, 11), (statistical, 9), (fisher, 6)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sampling</th>\n",
       "      <td>242</td>\n",
       "      <td>[(population, 7), (sample, 6), (theory, 5)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Experimental and observational studies</th>\n",
       "      <td>201</td>\n",
       "      <td>[(studies, 6), (variables, 4), (study, 4)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        words_count  \\\n",
       "section                                               \n",
       "Introduction                                    346   \n",
       "Mathematical statistics                          27   \n",
       "History                                         758   \n",
       "Sampling                                        242   \n",
       "Experimental and observational studies          201   \n",
       "\n",
       "                                                                                top words  \n",
       "section                                                                                    \n",
       "Introduction                               [(data, 16), (population, 8), (statistics, 7)]  \n",
       "Mathematical statistics                 [(mathematical, 3), (analysis, 2), (statistics...  \n",
       "History                                 [(statistics, 11), (statistical, 9), (fisher, 6)]  \n",
       "Sampling                                      [(population, 7), (sample, 6), (theory, 5)]  \n",
       "Experimental and observational studies         [(studies, 6), (variables, 4), (study, 4)]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load language model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "wiki_obj.df['top words'] = find_top_words(nlp, wiki_obj.df, n=3)\n",
    "print(wiki_obj.subject)\n",
    "wiki_obj.df[['words_count','top words']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d8c0ca",
   "metadata": {},
   "source": [
    "The dataframe above is from the 'Statistics' page on Wikipedia. Let's compare it to the Wikipedia page on ballet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1d34f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ballet\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words_count</th>\n",
       "      <th>top words</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>section</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Etymology</th>\n",
       "      <td>51</td>\n",
       "      <td>[(dance, 3), (french, 2), (word, 2)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>History</th>\n",
       "      <td>340</td>\n",
       "      <td>[(ballet, 11), (dance, 4), (louis, 3)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Styles</th>\n",
       "      <td>62</td>\n",
       "      <td>[(ballet, 9), (variations, 3), (classical, 2)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Classical ballet</th>\n",
       "      <td>120</td>\n",
       "      <td>[(ballet, 12), (classical, 3), (styles, 2)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Romantic ballet</th>\n",
       "      <td>155</td>\n",
       "      <td>[(romantic, 6), (ballet, 6), (era, 3)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  words_count                                       top words\n",
       "section                                                                      \n",
       "Etymology                  51            [(dance, 3), (french, 2), (word, 2)]\n",
       "History                   340          [(ballet, 11), (dance, 4), (louis, 3)]\n",
       "Styles                     62  [(ballet, 9), (variations, 3), (classical, 2)]\n",
       "Classical ballet          120     [(ballet, 12), (classical, 3), (styles, 2)]\n",
       "Romantic ballet           155          [(romantic, 6), (ballet, 6), (era, 3)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SUBJECT_2 = \"Ballet\"\n",
    "\n",
    "ballet = WikiScraper(SUBJECT_2)\n",
    "ballet.df['top words'] = find_top_words(nlp, ballet.df, n=3)\n",
    "print(ballet.subject)\n",
    "ballet.df[['words_count','top words']].head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
